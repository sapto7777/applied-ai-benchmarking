# Medical-LLM Benchmarking Artifact

This folder contains reusable benchmarking tables and evaluation notes derived from:

**Transforming Healthcare with State-of-the-Art Medical-LLMs: A Comprehensive Evaluation of Current Advances Using Benchmarking Framework**

## What’s here
- A reusable comparison table of Medical LLMs vs tasks/benchmarks
- Notes on evaluation metrics and clinical constraints

## Files
- `model_comparison_table.csv` — main benchmarking table
- `metrics_notes.md` — evaluation notes / guidance

## Citation
If you use this table or evaluation summary, please cite:

@Article{cmc.2025.070507,
AUTHOR = {Himadri Nath Saha, Dipanwita Chakraborty Bhattacharya, Sancharita Dutta, Arnab Bera, Srutorshi Basuray, Satyasaran Changdar, Saptarshi Banerjee, Jon Turdiev},
TITLE = {Transforming Healthcare with State-of-the-Art Medical-LLMs: A Comprehensive Evaluation of Current Advances Using Benchmarking Framework},
JOURNAL = {Computers, Materials \& Continua},
VOLUME = {86},
YEAR = {2026},
NUMBER = {2},
PAGES = {1--56},
URL = {http://www.techscience.com/cmc/v86n2/64752},
ISSN = {1546-2226},
ABSTRACT = {The emergence of Medical Large Language Models has significantly transformed healthcare. Medical Large Language Models (Med-LLMs) serve as transformative tools that enhance clinical practice through applications in decision support, documentation, and diagnostics. This evaluation examines the performance of leading Med-LLMs, including GPT-4Med, Med-PaLM, MEDITRON, PubMedGPT, and MedAlpaca, across diverse medical datasets. It provides graphical comparisons of their effectiveness in distinct healthcare domains. The study introduces a domain-specific categorization system that aligns these models with optimal applications in clinical decision-making, documentation, drug discovery, research, patient interaction, and public health. The paper addresses deployment challenges of Medical-LLMs, emphasizing trustworthiness and explainability as essential requirements for healthcare AI. It presents current evaluation techniques that improve model transparency in high-stakes medical contexts and analyzes regulatory frameworks using benchmarking datasets such as MedQA, MedMCQA, PubMedQA, and MIMIC. By identifying ongoing challenges in bias mitigation, reliability, and ethical compliance, this work serves as a resource for selecting appropriate Med-LLMs and outlines future directions in the field. This analysis offers a roadmap for developing Med-LLMs that balance technological innovation with the trust and transparency required for clinical integration, a perspective often overlooked in existing literature.},
DOI = {10.32604/cmc.2025.070507}
}

## Journal DOI
https://doi.org/10.32604/cmc.2025.070507

## Contact
Open an issue if you want a specific model/benchmark added.

